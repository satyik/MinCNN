{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üß† EGRR ‚Äî Entropy-Gated Recursive Residual Network\n",
                "\n",
                "**Target**: >70% Top-1 accuracy on CIFAR-100 with <500K parameters\n",
                "\n",
                "### Three Core Mechanisms\n",
                "1. **Symmetric Shared-Weight 1√ó1 Conv** ‚Äî `W = L + L·µÄ` for spectral stability\n",
                "2. **Entropy-Gated Dynamic Dilation** ‚Äî local variance selects dilation rate `d ‚àà {1, 2, 4}`\n",
                "3. **Iteration-Specific Normalization** ‚Äî per-recursion affine params `(Œ≥_t, Œ≤_t)`\n",
                "\n",
                "### Recursive Update Rule\n",
                "```\n",
                "h_t = h_{t-1} + ReLU6(IS-Norm_t(DWConv_gated(SymConv(h_{t-1}))))\n",
                "```\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 0. Setup & GPU Check"
            ],
            "metadata": {
                "id": "setup_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.checkpoint import checkpoint as grad_checkpoint\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import numpy as np\n",
                "import math\n",
                "import time\n",
                "import os\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"üñ•Ô∏è  Device: {device}\")\n",
                "if device.type == 'cuda':\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
                "    torch.backends.cudnn.benchmark = True"
            ],
            "metadata": {
                "id": "setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Configuration"
            ],
            "metadata": {
                "id": "config_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "# Configuration ‚Äî All hyperparameters\n",
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "\n",
                "class Config:\n",
                "    # Model variant\n",
                "    MODEL_VARIANT = \"base\"  # \"base\" or \"deep\"\n",
                "    NUM_CLASSES = 100\n",
                "    STEM_CHANNELS = 32\n",
                "\n",
                "    # Architecture stages: (channels, T_recursions, stride)\n",
                "    STAGES_BASE = [\n",
                "        (32,  3, 1),   # Stage 1: 32√ó32 ‚Üí 32√ó32\n",
                "        (64,  4, 2),   # Stage 2: 32√ó32 ‚Üí 16√ó16\n",
                "        (64,  4, 1),   # Stage 3: 16√ó16\n",
                "        (128, 5, 2),   # Stage 4: 16√ó16 ‚Üí 8√ó8\n",
                "        (128, 5, 1),   # Stage 5: 8√ó8\n",
                "        (128, 5, 1),   # Stage 6: 8√ó8\n",
                "        (256, 6, 2),   # Stage 7: 8√ó8 ‚Üí 4√ó4\n",
                "        (256, 6, 1),   # Stage 8: 4√ó4\n",
                "    ]\n",
                "    STAGES_DEEP = [\n",
                "        (32,  6,  1),  (64,  8,  2),  (64,  8,  1),\n",
                "        (128, 10, 2),  (128, 10, 1),  (128, 10, 1),\n",
                "        (256, 12, 2),  (256, 12, 1),\n",
                "    ]\n",
                "    WIDTH_MULT = 1.22\n",
                "\n",
                "    # Entropy Gate\n",
                "    DILATION_RATES = [1, 2, 4]\n",
                "    GUMBEL_TAU_START = 1.0\n",
                "    GUMBEL_TAU_END = 0.1\n",
                "\n",
                "    # Training\n",
                "    BATCH_SIZE = 128\n",
                "    EPOCHS = 200\n",
                "    LEARNING_RATE = 0.1\n",
                "    MOMENTUM = 0.9\n",
                "    WEIGHT_DECAY = 5e-4\n",
                "    LABEL_SMOOTHING = 0.1\n",
                "    LR_MIN = 0.0\n",
                "    LR_WARMUP_EPOCHS = 5\n",
                "    DEPTH_WARMUP_END_EPOCH = 20\n",
                "    CUTOUT_LENGTH = 8\n",
                "    USE_AUTOAUGMENT = True\n",
                "\n",
                "    # Memory optimization\n",
                "    USE_AMP = True\n",
                "    USE_GRADIENT_CHECKPOINTING = True\n",
                "\n",
                "    @property\n",
                "    def STAGES(self):\n",
                "        return self.STAGES_DEEP if self.MODEL_VARIANT == \"deep\" else self.STAGES_BASE\n",
                "\n",
                "cfg = Config()\n",
                "print(f\"Model variant: {cfg.MODEL_VARIANT}\")\n",
                "print(f\"Stages: {len(cfg.STAGES)}, Virtual depth: {sum(T for _, T, _ in cfg.STAGES)}\")"
            ],
            "metadata": {
                "id": "config"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## 2. Core Modules\n",
                "\n",
                "### 2.1 Symmetric Shared-Weight 1√ó1 Convolution\n",
                "\n",
                "Parameterized as `W = L + L·µÄ` where L is lower-triangular.  \n",
                "**Why?** Symmetric matrices have real eigenvalues ‚Üí stabilizes recursive weight sharing.  \n",
                "**Bonus:** Only `C(C+1)/2` unique parameters instead of `C¬≤`."
            ],
            "metadata": {
                "id": "symconv_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class SymmetricConv1x1(nn.Module):\n",
                "    \"\"\"Symmetric 1√ó1 pointwise convolution.\n",
                "\n",
                "    W = L_lower + L_lower^T guarantees:\n",
                "    - W is symmetric ‚Üí real eigenvalues\n",
                "    - Orthogonal init ‚Üí eigenvalues ‚âà 1 at start\n",
                "    - Fewer unique parameters: C(C+1)/2\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, channels: int):\n",
                "        super().__init__()\n",
                "        self.channels = channels\n",
                "        self.L = nn.Parameter(torch.empty(channels, channels))\n",
                "        self._init_orthogonal()\n",
                "        self.register_buffer(\"tril_mask\", torch.tril(torch.ones(channels, channels)))\n",
                "\n",
                "    def _init_orthogonal(self):\n",
                "        Q = torch.linalg.qr(torch.randn(self.channels, self.channels))[0]\n",
                "        with torch.no_grad():\n",
                "            self.L.copy_(Q / 2.0)\n",
                "        self._cached_weight = None\n",
                "\n",
                "    def get_weight(self):\n",
                "        L_lower = self.L * self.tril_mask\n",
                "        return L_lower + L_lower.transpose(0, 1)\n",
                "\n",
                "    def train(self, mode=True):\n",
                "        super().train(mode)\n",
                "        if mode:\n",
                "            self._cached_weight = None\n",
                "        return self\n",
                "\n",
                "    def forward(self, x):\n",
                "        if not self.training and self._cached_weight is not None:\n",
                "            weight = self._cached_weight\n",
                "        else:\n",
                "            weight = self.get_weight().unsqueeze(-1).unsqueeze(-1)\n",
                "            if not self.training:\n",
                "                self._cached_weight = weight\n",
                "        return F.conv2d(x, weight)\n",
                "\n",
                "    @property\n",
                "    def unique_params(self):\n",
                "        return self.channels * (self.channels + 1) // 2\n",
                "\n",
                "\n",
                "# ‚îÄ‚îÄ Quick test ‚îÄ‚îÄ\n",
                "sym = SymmetricConv1x1(64)\n",
                "W = sym.get_weight()\n",
                "print(f\"‚úÖ SymmetricConv1x1: W shape={W.shape}, symmetric={torch.allclose(W, W.T, atol=1e-7)}\")\n",
                "print(f\"   Unique params: {sym.unique_params} vs full: {64*64}\")"
            ],
            "metadata": {
                "id": "symmetric_conv"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 2.2 Iteration-Specific Normalization (IS-Norm)\n",
                "\n",
                "Each recursion step `t` gets its own affine `(Œ≥_t, Œ≤_t)`, but **shares** running mean/var.  \n",
                "Lets the network \"shift gears\" at each recursion without full BN cost.  \n",
                "**Cost**: Only `2 √ó T √ó C` extra params (negligible)."
            ],
            "metadata": {
                "id": "isnorm_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class ISNorm(nn.Module):\n",
                "    \"\"\"Iteration-Specific Batch Normalization.\n",
                "\n",
                "    Uses F.batch_norm for fused, memory-efficient normalization.\n",
                "    Per-iteration gamma[t] and beta[t] are passed directly.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, num_features: int, num_iterations: int,\n",
                "                 eps: float = 1e-5, momentum: float = 0.1):\n",
                "        super().__init__()\n",
                "        self.num_features = num_features\n",
                "        self.num_iterations = num_iterations\n",
                "        self.eps = eps\n",
                "        self.momentum = momentum\n",
                "\n",
                "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
                "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
                "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
                "\n",
                "        # Per-iteration affine: (T, C)\n",
                "        self.gamma = nn.Parameter(torch.ones(num_iterations, num_features))\n",
                "        self.beta = nn.Parameter(torch.zeros(num_iterations, num_features))\n",
                "\n",
                "    def forward(self, x, t: int):\n",
                "        assert 0 <= t < self.num_iterations\n",
                "        if self.training:\n",
                "            self.num_batches_tracked += 1\n",
                "        return F.batch_norm(\n",
                "            x, self.running_mean, self.running_var,\n",
                "            weight=self.gamma[t], bias=self.beta[t],\n",
                "            training=self.training, momentum=self.momentum, eps=self.eps,\n",
                "        )\n",
                "\n",
                "\n",
                "# ‚îÄ‚îÄ Quick test ‚îÄ‚îÄ\n",
                "norm = ISNorm(64, num_iterations=4)\n",
                "x_test = torch.randn(2, 64, 8, 8)\n",
                "norm.train()\n",
                "for t in range(4):\n",
                "    out = norm(x_test, t)\n",
                "print(f\"‚úÖ ISNorm: T=4, output shape={out.shape}, params={sum(p.numel() for p in norm.parameters())}\")"
            ],
            "metadata": {
                "id": "is_norm"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 2.3 Entropy-Gated Dynamic Dilation\n",
                "\n",
                "Uses **local variance** as a differentiable entropy proxy.  \n",
                "A lightweight decision head (`1√ó1 Conv ‚Üí ReLU ‚Üí 1√ó1 Conv`) maps variance to 3-way logits.  \n",
                "- **Training**: Gumbel-Softmax (œÑ annealed from 1.0 ‚Üí 0.1)\n",
                "- **Inference**: Hard argmax ‚Üí skip unused dilations"
            ],
            "metadata": {
                "id": "entropygate_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class EntropyGate(nn.Module):\n",
                "    \"\"\"Entropy-Gated Dynamic Dilation.\n",
                "\n",
                "    V(i,j) = AvgPool(X¬≤) - (AvgPool(X))¬≤  ‚Üí decision head ‚Üí gate weights\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, channels: int, pool_size: int = 3,\n",
                "                 num_dilations: int = 3, tau: float = 1.0):\n",
                "        super().__init__()\n",
                "        self.channels = channels\n",
                "        self.num_dilations = num_dilations\n",
                "        self.tau = tau\n",
                "\n",
                "        self.avg_pool = nn.AvgPool2d(pool_size, stride=1,\n",
                "                                     padding=pool_size // 2,\n",
                "                                     count_include_pad=False)\n",
                "        self.decision_head = nn.Sequential(\n",
                "            nn.Conv2d(channels, channels // 4, 1, bias=False),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(channels // 4, num_dilations, 1, bias=True),\n",
                "        )\n",
                "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
                "\n",
                "    def compute_local_variance(self, x):\n",
                "        \"\"\"V = E[X¬≤] - E[X]¬≤ (memory-optimized)\"\"\"\n",
                "        ex = self.avg_pool(x)\n",
                "        e_x2 = self.avg_pool(x.square())\n",
                "        return (e_x2 - ex.square()).clamp_(min=0.0)\n",
                "\n",
                "    def forward(self, x):\n",
                "        variance = self.compute_local_variance(x)\n",
                "        logits = self.decision_head(variance)\n",
                "        del variance\n",
                "        logits = self.global_pool(logits).squeeze(-1).squeeze(-1)  # (N, 3)\n",
                "\n",
                "        if self.training:\n",
                "            gate_weights = F.gumbel_softmax(logits, tau=self.tau, hard=False, dim=-1)\n",
                "        else:\n",
                "            gate_weights = F.one_hot(\n",
                "                logits.argmax(dim=-1), num_classes=self.num_dilations\n",
                "            ).float()\n",
                "\n",
                "        return gate_weights.unsqueeze(-1).unsqueeze(-1)  # (N, 3, 1, 1)\n",
                "\n",
                "    def set_tau(self, tau):\n",
                "        self.tau = tau\n",
                "\n",
                "\n",
                "# ‚îÄ‚îÄ Quick test ‚îÄ‚îÄ\n",
                "gate = EntropyGate(64)\n",
                "gate.eval()\n",
                "with torch.no_grad():\n",
                "    w = gate(torch.randn(4, 64, 16, 16))\n",
                "print(f\"‚úÖ EntropyGate: weights shape={w.shape}, sum={w.squeeze().sum(dim=-1)}\")"
            ],
            "metadata": {
                "id": "entropy_gate"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 2.4 Shared Depthwise Conv & EGRR Block\n",
                "\n",
                "**SharedDepthwiseConv**: Single 3√ó3 depthwise kernel reused at all dilation rates.  \n",
                "Memory-optimized: accumulates weighted sum in-place, skips unused dilations at inference.\n",
                "\n",
                "**EGRRBlock**: The core recursive block with optional gradient checkpointing.  \n",
                "```\n",
                "h_t = h_{t-1} + ReLU6(IS-Norm_t(DWConv_gated(SymConv(h_{t-1}))))\n",
                "```"
            ],
            "metadata": {
                "id": "block_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class SharedDepthwiseConv(nn.Module):\n",
                "    \"\"\"Single shared depthwise weight, reused for all dilation rates.\"\"\"\n",
                "\n",
                "    def __init__(self, channels, kernel_size=3, dilation_rates=None):\n",
                "        super().__init__()\n",
                "        self.channels = channels\n",
                "        self.kernel_size = kernel_size\n",
                "        self.dilation_rates = dilation_rates or [1, 2, 4]\n",
                "        self.weight = nn.Parameter(torch.randn(channels, 1, kernel_size, kernel_size) * 0.02)\n",
                "        self.bias = nn.Parameter(torch.zeros(channels))\n",
                "\n",
                "    def forward(self, x, gate_weights):\n",
                "        result = None\n",
                "        for i, d in enumerate(self.dilation_rates):\n",
                "            w = gate_weights[:, i:i+1]\n",
                "            if not self.training and w.sum().item() == 0:\n",
                "                continue  # Skip unused dilations\n",
                "            padding = d * (self.kernel_size // 2)\n",
                "            out = F.conv2d(x, self.weight, self.bias,\n",
                "                          stride=1, padding=padding, dilation=d,\n",
                "                          groups=self.channels)\n",
                "            if result is None:\n",
                "                result = out * w\n",
                "            else:\n",
                "                result = result + out * w\n",
                "        return result\n",
                "\n",
                "\n",
                "class EGRRBlock(nn.Module):\n",
                "    \"\"\"Entropy-Gated Recursive Residual Block.\n",
                "\n",
                "    Recursive loop of T iterations with shared weights,\n",
                "    entropy-gated dilation, and per-iteration normalization.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, in_channels, out_channels, num_iterations=4,\n",
                "                 stride=1, kernel_size=3, dilation_rates=None, pool_size=3):\n",
                "        super().__init__()\n",
                "        self.in_channels = in_channels\n",
                "        self.out_channels = out_channels\n",
                "        self.num_iterations = num_iterations\n",
                "        self.stride = stride\n",
                "        self.use_projection = (in_channels != out_channels) or (stride != 1)\n",
                "\n",
                "        # Channel projection\n",
                "        if self.use_projection:\n",
                "            self.projection = nn.Sequential(\n",
                "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
                "                nn.BatchNorm2d(out_channels),\n",
                "            )\n",
                "            self.downsample = nn.AvgPool2d(2, 2) if stride == 2 else nn.Identity()\n",
                "        else:\n",
                "            self.projection = nn.Identity()\n",
                "            self.downsample = nn.Identity()\n",
                "\n",
                "        # Three mechanisms\n",
                "        self.sym_conv = SymmetricConv1x1(out_channels)\n",
                "        self.entropy_gate = EntropyGate(out_channels, pool_size,\n",
                "                                        len(dilation_rates or [1, 2, 4]))\n",
                "        self.shared_dw_conv = SharedDepthwiseConv(out_channels, kernel_size,\n",
                "                                                  dilation_rates)\n",
                "        self.is_norm = ISNorm(out_channels, num_iterations)\n",
                "        self.activation = nn.ReLU6(inplace=True)\n",
                "\n",
                "        self._active_iterations = num_iterations\n",
                "        self._use_gradient_checkpointing = False\n",
                "\n",
                "    @property\n",
                "    def active_iterations(self):\n",
                "        return self._active_iterations\n",
                "\n",
                "    @active_iterations.setter\n",
                "    def active_iterations(self, value):\n",
                "        self._active_iterations = min(value, self.num_iterations)\n",
                "\n",
                "    def _recursive_step(self, h, t):\n",
                "        z = self.sym_conv(h)\n",
                "        gate_weights = self.entropy_gate(h)\n",
                "        z = self.shared_dw_conv(z, gate_weights)\n",
                "        z = self.is_norm(z, t)\n",
                "        z = self.activation(z)\n",
                "        return h + z\n",
                "\n",
                "    def forward(self, x):\n",
                "        if self.stride == 2:\n",
                "            x = self.downsample(x)\n",
                "        h = self.projection(x)\n",
                "\n",
                "        T = self._active_iterations\n",
                "        use_ckpt = self.training and self._use_gradient_checkpointing and T > 1\n",
                "\n",
                "        for t in range(T):\n",
                "            if use_ckpt:\n",
                "                h = grad_checkpoint(self._recursive_step, h, t,\n",
                "                                    use_reentrant=False)\n",
                "            else:\n",
                "                h = self._recursive_step(h, t)\n",
                "        return h\n",
                "\n",
                "\n",
                "# ‚îÄ‚îÄ Quick test ‚îÄ‚îÄ\n",
                "block = EGRRBlock(32, 64, num_iterations=3, stride=2, dilation_rates=[1, 2, 4])\n",
                "out = block(torch.randn(2, 32, 16, 16))\n",
                "print(f\"‚úÖ EGRRBlock: 32‚Üí64, stride=2, T=3 ‚Üí output={out.shape}\")\n",
                "print(f\"   Params: {sum(p.numel() for p in block.parameters()):,}\")"
            ],
            "metadata": {
                "id": "egrr_block"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 2.5 Complete EGRR Network\n",
                "\n",
                "Pyramidal structure: 8 stages with decreasing resolution and increasing channels.  \n",
                "Stem ‚Üí 8 EGRR Blocks ‚Üí Global AvgPool ‚Üí Dropout ‚Üí Linear(100)"
            ],
            "metadata": {
                "id": "net_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class EGRRNet(nn.Module):\n",
                "    \"\"\"Entropy-Gated Recursive Residual Network for CIFAR-100.\"\"\"\n",
                "\n",
                "    def __init__(self, num_classes=100, stages=None, stem_channels=32,\n",
                "                 width_mult=1.5, dilation_rates=None):\n",
                "        super().__init__()\n",
                "        self.num_classes = num_classes\n",
                "        if dilation_rates is None:\n",
                "            dilation_rates = [1, 2, 4]\n",
                "        if stages is None:\n",
                "            stages = cfg.STAGES_BASE\n",
                "\n",
                "        # Stem\n",
                "        self.stem = nn.Sequential(\n",
                "            nn.Conv2d(3, stem_channels, 3, 1, 1, bias=False),\n",
                "            nn.BatchNorm2d(stem_channels),\n",
                "            nn.ReLU6(inplace=True),\n",
                "        )\n",
                "\n",
                "        # EGRR Stages\n",
                "        self.stages = nn.ModuleList()\n",
                "        in_ch = stem_channels\n",
                "        for base_ch, T, stride in stages:\n",
                "            out_ch = self._scale(base_ch, width_mult)\n",
                "            self.stages.append(EGRRBlock(\n",
                "                in_ch, out_ch, T, stride, dilation_rates=dilation_rates))\n",
                "            in_ch = out_ch\n",
                "\n",
                "        # Head\n",
                "        self.head = nn.Sequential(\n",
                "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
                "            nn.Dropout(0.1), nn.Linear(in_ch, num_classes),\n",
                "        )\n",
                "        self.last_channels = in_ch\n",
                "\n",
                "    @staticmethod\n",
                "    def _scale(base_c, width_mult):\n",
                "        return max(8, int(round(base_c * width_mult / 8) * 8))\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.stem(x)\n",
                "        for stage in self.stages:\n",
                "            x = stage(x)\n",
                "        return self.head(x)\n",
                "\n",
                "    def set_active_iterations(self, max_t):\n",
                "        for s in self.stages:\n",
                "            s.active_iterations = max_t\n",
                "\n",
                "    def set_gumbel_tau(self, tau):\n",
                "        for s in self.stages:\n",
                "            s.entropy_gate.set_tau(tau)\n",
                "\n",
                "    def set_gradient_checkpointing(self, enable=True):\n",
                "        for s in self.stages:\n",
                "            s._use_gradient_checkpointing = enable\n",
                "\n",
                "    def count_parameters(self):\n",
                "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
                "\n",
                "    def parameter_breakdown(self):\n",
                "        bd = {\n",
                "            \"stem\": sum(p.numel() for p in self.stem.parameters()),\n",
                "            \"head\": sum(p.numel() for p in self.head.parameters()),\n",
                "            \"stages\": {}, \"total\": self.count_parameters(),\n",
                "        }\n",
                "        for i, s in enumerate(self.stages):\n",
                "            sp = {\n",
                "                \"sym_conv\": sum(p.numel() for p in s.sym_conv.parameters()),\n",
                "                \"entropy_gate\": sum(p.numel() for p in s.entropy_gate.parameters()),\n",
                "                \"shared_dw_conv\": sum(p.numel() for p in s.shared_dw_conv.parameters()),\n",
                "                \"is_norm\": sum(p.numel() for p in s.is_norm.parameters()),\n",
                "                \"projection\": sum(p.numel() for p in s.projection.parameters()) if s.use_projection else 0,\n",
                "            }\n",
                "            sp[\"subtotal\"] = sum(sp.values())\n",
                "            bd[\"stages\"][f\"stage_{i+1}\"] = sp\n",
                "        return bd\n",
                "\n",
                "\n",
                "# ‚îÄ‚îÄ Build & verify ‚îÄ‚îÄ\n",
                "model = EGRRNet(\n",
                "    num_classes=cfg.NUM_CLASSES,\n",
                "    stages=cfg.STAGES,\n",
                "    stem_channels=cfg.STEM_CHANNELS,\n",
                "    width_mult=cfg.WIDTH_MULT,\n",
                "    dilation_rates=cfg.DILATION_RATES,\n",
                ").to(device)\n",
                "\n",
                "total = model.count_parameters()\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"EGRR Network ‚Äî {total:,} parameters\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "bd = model.parameter_breakdown()\n",
                "print(f\"  Stem:   {bd['stem']:>8,}\")\n",
                "for name, info in bd['stages'].items():\n",
                "    print(f\"  {name}: {info['subtotal']:>8,}\")\n",
                "print(f\"  Head:   {bd['head']:>8,}\")\n",
                "print(f\"  Total:  {bd['total']:>8,}\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\\n‚úÖ Under 500K budget: {total:,} < 500,000 ‚Üí {'PASS' if total < 500_000 else 'FAIL'}\")\n",
                "\n",
                "# Forward pass test\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    out = model(torch.randn(2, 3, 32, 32).to(device))\n",
                "print(f\"‚úÖ Forward: (2, 3, 32, 32) ‚Üí {out.shape}\")"
            ],
            "metadata": {
                "id": "egrr_net"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## 3. Utilities"
            ],
            "metadata": {
                "id": "utils_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "# Training Utilities\n",
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "\n",
                "def init_weights(model):\n",
                "    for name, m in model.named_modules():\n",
                "        if isinstance(m, nn.Conv2d):\n",
                "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
                "            if m.bias is not None:\n",
                "                nn.init.zeros_(m.bias)\n",
                "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
                "            nn.init.ones_(m.weight)\n",
                "            nn.init.zeros_(m.bias)\n",
                "        elif isinstance(m, nn.Linear):\n",
                "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
                "            if m.bias is not None:\n",
                "                nn.init.zeros_(m.bias)\n",
                "\n",
                "\n",
                "class AverageMeter:\n",
                "    def __init__(self):\n",
                "        self.reset()\n",
                "    def reset(self):\n",
                "        self.val = self.avg = self.sum = 0.0\n",
                "        self.count = 0\n",
                "    def update(self, val, n=1):\n",
                "        self.val = val\n",
                "        self.sum += val * n\n",
                "        self.count += n\n",
                "        self.avg = self.sum / self.count\n",
                "\n",
                "\n",
                "def accuracy(output, target, topk=(1, 5)):\n",
                "    with torch.no_grad():\n",
                "        maxk = max(topk)\n",
                "        bs = target.size(0)\n",
                "        _, pred = output.topk(maxk, 1, True, True)\n",
                "        correct = pred.t().eq(target.view(1, -1).expand_as(pred.t()))\n",
                "        return [correct[:k].reshape(-1).float().sum(0).mul_(100.0/bs).item() for k in topk]\n",
                "\n",
                "\n",
                "class Cutout:\n",
                "    def __init__(self, length):\n",
                "        self.length = length\n",
                "    def __call__(self, img):\n",
                "        if self.length <= 0:\n",
                "            return img\n",
                "        h, w = img.size(1), img.size(2)\n",
                "        mask = np.ones((h, w), np.float32)\n",
                "        y, x = np.random.randint(h), np.random.randint(w)\n",
                "        y1, y2 = np.clip(y - self.length//2, 0, h), np.clip(y + self.length//2, 0, h)\n",
                "        x1, x2 = np.clip(x - self.length//2, 0, w), np.clip(x + self.length//2, 0, w)\n",
                "        mask[y1:y2, x1:x2] = 0.0\n",
                "        return img * torch.from_numpy(mask).expand_as(img)\n",
                "\n",
                "\n",
                "def get_active_iterations(epoch, warmup_start=0, warmup_end=20, max_T=10):\n",
                "    if epoch <= warmup_start:\n",
                "        return 1\n",
                "    elif epoch >= warmup_end:\n",
                "        return max_T\n",
                "    progress = (epoch - warmup_start) / (warmup_end - warmup_start)\n",
                "    return max(1, int(math.ceil(progress * max_T)))\n",
                "\n",
                "\n",
                "def get_gumbel_tau(epoch, total_epochs, tau_start=1.0, tau_end=0.1):\n",
                "    progress = min(1.0, epoch / max(1, total_epochs))\n",
                "    return max(tau_end, tau_start * (tau_end / tau_start) ** progress)\n",
                "\n",
                "\n",
                "def cosine_lr(optimizer, epoch, total_epochs, lr_max, lr_min=0.0, warmup_epochs=5):\n",
                "    if epoch < warmup_epochs:\n",
                "        lr = lr_max * (epoch + 1) / warmup_epochs\n",
                "    else:\n",
                "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
                "        lr = lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * progress))\n",
                "    for pg in optimizer.param_groups:\n",
                "        pg[\"lr\"] = lr\n",
                "    return lr\n",
                "\n",
                "\n",
                "print(\"‚úÖ Utilities loaded\")"
            ],
            "metadata": {
                "id": "utils"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## 4. Data Loaders"
            ],
            "metadata": {
                "id": "data_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def get_dataloaders(batch_size=128, num_workers=2):\n",
                "    mean = (0.5071, 0.4867, 0.4408)\n",
                "    std  = (0.2675, 0.2565, 0.2761)\n",
                "\n",
                "    train_tf = [\n",
                "        transforms.RandomCrop(32, padding=4),\n",
                "        transforms.RandomHorizontalFlip(),\n",
                "    ]\n",
                "    if cfg.USE_AUTOAUGMENT:\n",
                "        train_tf.append(transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10))\n",
                "    train_tf += [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
                "    if cfg.CUTOUT_LENGTH > 0:\n",
                "        train_tf.append(Cutout(cfg.CUTOUT_LENGTH))\n",
                "\n",
                "    test_tf = [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
                "\n",
                "    use_cuda = device.type == 'cuda'\n",
                "    train_ds = torchvision.datasets.CIFAR100('./data', train=True,  download=True,\n",
                "                                             transform=transforms.Compose(train_tf))\n",
                "    test_ds  = torchvision.datasets.CIFAR100('./data', train=False, download=True,\n",
                "                                             transform=transforms.Compose(test_tf))\n",
                "    train_loader = torch.utils.data.DataLoader(\n",
                "        train_ds, batch_size=batch_size, shuffle=True,\n",
                "        num_workers=num_workers, pin_memory=use_cuda, drop_last=True)\n",
                "    test_loader = torch.utils.data.DataLoader(\n",
                "        test_ds, batch_size=batch_size, shuffle=False,\n",
                "        num_workers=num_workers, pin_memory=use_cuda)\n",
                "    return train_loader, test_loader\n",
                "\n",
                "\n",
                "train_loader, test_loader = get_dataloaders(cfg.BATCH_SIZE)\n",
                "print(f\"‚úÖ CIFAR-100: {len(train_loader.dataset)} train, {len(test_loader.dataset)} test\")"
            ],
            "metadata": {
                "id": "dataloaders"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## 5. Training Loop\n",
                "\n",
                "Features:\n",
                "- **Depth warm-up**: T increases from 1 ‚Üí max over first 20 epochs\n",
                "- **Gumbel-œÑ annealing**: 1.0 ‚Üí 0.1 (exponential decay)\n",
                "- **Cosine LR** with 5-epoch linear warmup\n",
                "- **AMP** mixed precision on CUDA\n",
                "- **Gradient checkpointing** for memory efficiency"
            ],
            "metadata": {
                "id": "training_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def train_one_epoch(model, loader, criterion, optimizer, device,\n",
                "                    scaler=None, use_amp=False):\n",
                "    model.train()\n",
                "    losses, top1, top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
                "    amp_type = device.type if device.type in ('cuda', 'cpu') else 'cpu'\n",
                "\n",
                "    for i, (images, targets) in enumerate(loader):\n",
                "        images = images.to(device, non_blocking=True)\n",
                "        targets = targets.to(device, non_blocking=True)\n",
                "\n",
                "        with torch.amp.autocast(device_type=amp_type, enabled=use_amp):\n",
                "            logits = model(images)\n",
                "            loss = criterion(logits, targets)\n",
                "\n",
                "        optimizer.zero_grad(set_to_none=True)\n",
                "        if scaler:\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.unscale_(optimizer)\n",
                "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "        else:\n",
                "            loss.backward()\n",
                "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "            optimizer.step()\n",
                "\n",
                "        a1, a5 = accuracy(logits.float(), targets)\n",
                "        losses.update(loss.item(), images.size(0))\n",
                "        top1.update(a1, images.size(0))\n",
                "        top5.update(a5, images.size(0))\n",
                "\n",
                "    return losses.avg, top1.avg, top5.avg\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def evaluate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    losses, top1, top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
                "    for images, targets in loader:\n",
                "        images = images.to(device, non_blocking=True)\n",
                "        targets = targets.to(device, non_blocking=True)\n",
                "        logits = model(images)\n",
                "        loss = criterion(logits, targets)\n",
                "        a1, a5 = accuracy(logits, targets)\n",
                "        losses.update(loss.item(), images.size(0))\n",
                "        top1.update(a1, images.size(0))\n",
                "        top5.update(a5, images.size(0))\n",
                "    return losses.avg, top1.avg, top5.avg\n",
                "\n",
                "print(\"‚úÖ Training functions defined\")"
            ],
            "metadata": {
                "id": "training_funcs"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 5.1 Run Training"
            ],
            "metadata": {
                "id": "run_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "# Train!\n",
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "\n",
                "# Re-init model\n",
                "model = EGRRNet(\n",
                "    num_classes=cfg.NUM_CLASSES,\n",
                "    stages=cfg.STAGES,\n",
                "    stem_channels=cfg.STEM_CHANNELS,\n",
                "    width_mult=cfg.WIDTH_MULT,\n",
                "    dilation_rates=cfg.DILATION_RATES,\n",
                ").to(device)\n",
                "init_weights(model)\n",
                "\n",
                "if cfg.USE_GRADIENT_CHECKPOINTING:\n",
                "    model.set_gradient_checkpointing(True)\n",
                "    print(\"Gradient checkpointing: ENABLED\")\n",
                "\n",
                "max_T = max(T for _, T, _ in cfg.STAGES)\n",
                "criterion = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING)\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr=cfg.LEARNING_RATE,\n",
                "                            momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY,\n",
                "                            nesterov=True)\n",
                "\n",
                "use_amp = cfg.USE_AMP and device.type == 'cuda'\n",
                "scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
                "print(f\"AMP: {'ENABLED' if use_amp else 'DISABLED'}\")\n",
                "\n",
                "best_acc = 0.0\n",
                "history = []\n",
                "\n",
                "EPOCHS = cfg.EPOCHS  # Change this for faster testing\n",
                "# EPOCHS = 10  # Uncomment for quick test run\n",
                "\n",
                "print(f\"\\nüöÄ Training for {EPOCHS} epochs on {device}...\\n\")\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    t0 = time.time()\n",
                "\n",
                "    # Schedules\n",
                "    active_T = get_active_iterations(epoch, 0, cfg.DEPTH_WARMUP_END_EPOCH, max_T)\n",
                "    model.set_active_iterations(active_T)\n",
                "    tau = get_gumbel_tau(epoch, EPOCHS, cfg.GUMBEL_TAU_START, cfg.GUMBEL_TAU_END)\n",
                "    model.set_gumbel_tau(tau)\n",
                "    lr = cosine_lr(optimizer, epoch, EPOCHS, cfg.LEARNING_RATE,\n",
                "                   cfg.LR_MIN, cfg.LR_WARMUP_EPOCHS)\n",
                "\n",
                "    # Train & Eval\n",
                "    train_loss, train_acc, _ = train_one_epoch(\n",
                "        model, train_loader, criterion, optimizer, device, scaler, use_amp)\n",
                "    test_loss, test_acc, test_acc5 = evaluate(\n",
                "        model, test_loader, criterion, device)\n",
                "\n",
                "    elapsed = time.time() - t0\n",
                "    is_best = test_acc > best_acc\n",
                "    if is_best:\n",
                "        best_acc = test_acc\n",
                "\n",
                "    history.append({\"epoch\": epoch, \"train_acc\": train_acc,\n",
                "                    \"test_acc\": test_acc, \"test_acc5\": test_acc5})\n",
                "\n",
                "    if (epoch + 1) % 10 == 0 or is_best or epoch == 0:\n",
                "        star = \" ‚òÖ\" if is_best else \"\"\n",
                "        print(f\"Ep {epoch+1:3d}/{EPOCHS}  T={active_T}  œÑ={tau:.3f}  lr={lr:.5f}  \"\n",
                "              f\"Train={train_acc:.1f}%  Test={test_acc:.1f}% (Top5={test_acc5:.1f}%)  \"\n",
                "              f\"{elapsed:.1f}s{star}\")\n",
                "\n",
                "    # Save best\n",
                "    if is_best:\n",
                "        torch.save({\n",
                "            \"epoch\": epoch, \"model_state_dict\": model.state_dict(),\n",
                "            \"best_acc\": best_acc, \"stages\": cfg.STAGES,\n",
                "            \"width_mult\": cfg.WIDTH_MULT,\n",
                "        }, \"best_egrr.pth\")\n",
                "\n",
                "print(f\"\\nüèÜ Training complete! Best Top-1: {best_acc:.2f}%\")"
            ],
            "metadata": {
                "id": "train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## 6. Training Curves"
            ],
            "metadata": {
                "id": "plots_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if history:\n",
                "    epochs = [h['epoch'] for h in history]\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "    ax1.plot(epochs, [h['train_acc'] for h in history], label='Train', alpha=0.8)\n",
                "    ax1.plot(epochs, [h['test_acc'] for h in history], label='Test', alpha=0.8)\n",
                "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy (%)')\n",
                "    ax1.set_title('Top-1 Accuracy'); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
                "\n",
                "    ax2.plot(epochs, [h['test_acc5'] for h in history], color='green', alpha=0.8)\n",
                "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)')\n",
                "    ax2.set_title('Top-5 Accuracy'); ax2.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    print(f\"Best Top-1: {best_acc:.2f}%\")"
            ],
            "metadata": {
                "id": "plots"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## 7. Architecture Tests\n",
                "\n",
                "Quick verification of all core invariants."
            ],
            "metadata": {
                "id": "tests_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"Running architecture verification tests...\\n\")\n",
                "\n",
                "# Test 1: Parameter count\n",
                "m = EGRRNet(stages=cfg.STAGES, width_mult=cfg.WIDTH_MULT,\n",
                "            stem_channels=cfg.STEM_CHANNELS, dilation_rates=cfg.DILATION_RATES)\n",
                "total = m.count_parameters()\n",
                "assert total < 500_000, f\"FAIL: {total:,} >= 500K\"\n",
                "print(f\"‚úÖ Test 1: Parameter count = {total:,} < 500K\")\n",
                "\n",
                "# Test 2: Forward pass shape\n",
                "m.eval()\n",
                "with torch.no_grad():\n",
                "    out = m(torch.randn(2, 3, 32, 32))\n",
                "assert out.shape == (2, 100)\n",
                "assert torch.isfinite(out).all()\n",
                "print(f\"‚úÖ Test 2: Forward (2,3,32,32) ‚Üí {out.shape}\")\n",
                "\n",
                "# Test 3: IS-Norm shapes\n",
                "n = ISNorm(64, 4); n.train()\n",
                "x = torch.randn(2, 64, 8, 8)\n",
                "for t in range(4):\n",
                "    assert n(x, t).shape == x.shape\n",
                "print(\"‚úÖ Test 3: IS-Norm shapes correct for all T\")\n",
                "\n",
                "# Test 4: Entropy gate sums to 1\n",
                "g = EntropyGate(64); g.eval()\n",
                "with torch.no_grad():\n",
                "    w = g(torch.randn(4, 64, 16, 16))\n",
                "assert w.shape == (4, 3, 1, 1)\n",
                "assert torch.allclose(w.squeeze().sum(dim=-1), torch.ones(4), atol=1e-5)\n",
                "print(\"‚úÖ Test 4: Entropy gate weights sum to 1\")\n",
                "\n",
                "# Test 5: Symmetric kernel\n",
                "s = SymmetricConv1x1(64)\n",
                "W = s.get_weight()\n",
                "assert (W - W.T).abs().max() < 1e-7\n",
                "print(\"‚úÖ Test 5: W == W·µÄ (exact symmetry)\")\n",
                "\n",
                "# Test 6: Gradient flow\n",
                "b = EGRRBlock(32, 32, 3, 1, dilation_rates=[1, 2, 4])\n",
                "x = torch.randn(2, 32, 8, 8, requires_grad=True)\n",
                "b(x).sum().backward()\n",
                "assert x.grad is not None and (x.grad.abs() > 0).any()\n",
                "assert b.sym_conv.L.grad is not None\n",
                "print(\"‚úÖ Test 6: Gradients flow through recursive loop\")\n",
                "\n",
                "# Test 7: Depth warm-up\n",
                "m.set_active_iterations(1)\n",
                "assert all(s.active_iterations == 1 for s in m.stages)\n",
                "m.set_active_iterations(100)\n",
                "assert all(s.active_iterations == s.num_iterations for s in m.stages)\n",
                "print(\"‚úÖ Test 7: Depth warm-up capping works\")\n",
                "\n",
                "print(\"\\nüéâ All tests passed!\")"
            ],
            "metadata": {
                "id": "tests"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}